import os
import sys
import time
import streamlit as st
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from langchain.schema import HumanMessage, AIMessage
from langchain_community.chat_models import ChatOpenAI

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.pathï¼Œç¡®ä¿ query_data å¯è¢«å¯¼å…¥
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from query_data.Answer import CarQueryProcessor  # æ­¤æ—¶å¿…é¡»ä¿è¯ query_data ç›®å½•æœ‰ __init__.py


# ==== åˆå§‹åŒ– processorï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ ====
if "processor" not in st.session_state:
    st.session_state.processor = CarQueryProcessor(
        neo4j_url="bolt://localhost:7687",
        username="neo4j",
        password="neo4j0000",
        model_path=r"..\models\models--moka-ai--m3e-base\snapshots\764b537a0e50e5c7d64db883f2d2e051cbe3c64c",
        body_index="../data/body_car_vectors_ip.index",
        body_json="../data/body_car_name.json",
        object_index="../data/object_car_vectors_ip.index",
        object_json="../data/object_car_name.json",
        body_path="../data/body_field_map.json",
        object_path="../data/object_field_map.json"
    )

# ==== åˆå§‹åŒ– LangChain æ¨¡å‹ ====
if "llm" not in st.session_state:
    st.session_state.llm = ChatOpenAI(
        openai_api_key=os.getenv("DASHSCOPE_API_KEY"),
        openai_api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
        model_name="qwen-plus-2025-04-28"
    )

# ==== åˆå§‹åŒ– Streamlit é¡µé¢ ====
st.set_page_config(page_title="ğŸš— æ™ºèƒ½æ±½è½¦é—®ç­”ç³»ç»Ÿ", layout="wide")
st.title("ğŸš— æ™ºèƒ½æ±½è½¦é—®ç­”ç³»ç»Ÿ")
st.markdown("##### æ‚¨çš„ä¸“ä¸šæ±½è½¦å’¨è¯¢åŠ©æ‰‹ï¼Œä¸ºæ‚¨è§£ç­”å„ç±»æ±½è½¦ç›¸å…³é—®é¢˜")

# ==== åˆå§‹åŒ–èŠå¤©å†å²ä¸ memory ====
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ğŸ‘‹ æ‚¨å¥½ï¼æˆ‘æ˜¯æ±½è½¦é—®ç­”åŠ©æ‰‹ã€‚\næˆ‘å¯ä»¥å›ç­”å…³äºæ±½è½¦çš„å„ç§é—®é¢˜ï¼Œä¾‹å¦‚ï¼š\n- å¥”é©°Eçº§ 2025æ¬¾ æ”¹æ¬¾ E 260 L çš„åŸºæœ¬å‚æ•°\n- å®é©¬3ç³»çš„åŠ¨åŠ›æ€§èƒ½å¦‚ä½•\nè¯·é—®æ‚¨æƒ³äº†è§£ä»€ä¹ˆï¼Ÿ"}
    ]

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferMemory(return_messages=True)

# ==== ConversationChain æ”¯æŒå¤šè½®å¯¹è¯ ====
conversation = ConversationChain(
    llm=st.session_state.llm,
    memory=st.session_state.memory,
    verbose=True
)

# ==== æ˜¾ç¤ºå†å²è®°å½• ====
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).markdown(msg["content"])

# ==== ç”¨æˆ·è¾“å…¥ ====
user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜")

if user_input:
    st.chat_message("user").markdown(user_input)
    st.session_state.messages.append({"role": "user", "content": user_input})

    try:
        start = time.time()
        prompt = st.session_state.processor.batch_query_from_question(user_input)
        answer = st.session_state.processor.llm_answer(prompt)
        print(f"[INFO] æ€»è€—æ—¶: {time.time() - start:.2f}s")
    except Exception as e:
        answer = f"[ERROR] å¤„ç†é—®é¢˜æ—¶å‡ºé”™ï¼š{str(e)}"

    # å­˜å…¥ LangChain memoryï¼ˆæ”¯æŒå¤šè½®ä¸Šä¸‹æ–‡ï¼‰
    st.session_state.memory.chat_memory.add_user_message(user_input)
    st.session_state.memory.chat_memory.add_ai_message(answer)

    # æ˜¾ç¤ºåŠ©æ‰‹å›å¤
    st.chat_message("assistant").markdown(answer)
    st.session_state.messages.append({"role": "assistant", "content": answer})

# ==== æ¸…ç©ºå¯¹è¯æŒ‰é’® ====
if st.button("ğŸ§¹ æ¸…ç©ºå¯¹è¯", use_container_width=True):
    st.session_state.messages = []
    st.session_state.memory = ConversationBufferMemory(return_messages=True)
    st.experimental_rerun()
