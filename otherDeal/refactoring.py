from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# è·¯å¾„é…ç½®
base_model_path = "../models/Qwen-llm1.5-1.8B"  # åŸå§‹æ¨¡å‹ï¼Œå¯ä»¥æ˜¯æœ¬åœ°è·¯å¾„æˆ– Hugging Face å
lora_path = "../models/Lora_Qwen_llm"  # LoRA å¾®è°ƒä¿å­˜çš„è·¯å¾„
output_path = "../models/Qwen-llm1.5-1.8b-merge"  # åˆå¹¶åçš„ä¿å­˜è·¯å¾„

# åŠ è½½åŸæ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(base_model_path, torch_dtype="auto")

# åŠ è½½ LoRA å¹¶åˆå¹¶
model = PeftModel.from_pretrained(model, lora_path)
model = model.merge_and_unload()  # ğŸš¨ å…³é”®ï¼šåˆå¹¶ LoRA æƒé‡

# ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
model.save_pretrained(output_path)

# åŒæ—¶ä¿å­˜ tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.save_pretrained(output_path)
